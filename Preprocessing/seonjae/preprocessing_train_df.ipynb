{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ae0be2",
   "metadata": {},
   "source": [
    "# 서로 다른 모델과 분할된 데이터, 그 결과에 대한 병합\n",
    "---\n",
    "\n",
    "## 개요\n",
    "승순님이 만들어주신 100개의 chunk파일 중 일부를 사용하여 모델에 바로 입력할 수 있도록 데이터 파이프라인을 생성합니다.  \n",
    "데이터 파이프라인을 통과한 chunk file들로 CNN, DenseNet, ConvMixer 모델을 학습시킵니다. 각각의 모델을 사용하는 이유는 다음과 같습니다.  \n",
    "* **CNN**: CNN 모델을 통해 이미지의 국소적인 특징을 추출하기 위함입니다.\n",
    "* **DenseNet**: DenseNet 모델을 통해 이미지의 전체적인 특징을 추출하기 위함입니다.\n",
    "* **ConvMixer**: ConvMixer 모델을 통해 이미지와 토큰의 관계에 대한 보다 정확한 특징을 추출하기 위함입니다.  \n",
    "\n",
    "생성된 모델들은 아키텍처가 다르기 때문에 Weighted Average 앙상블을 진행하여 각 모델의 예측에 가중치를 부여하여 각각의 결과를 결합합니다.  \n",
    "일련의 과정들을 모듈화하여 각기 다른 chunk file로 학습을 진행하여 그 결과를 비교합니다.\n",
    "\n",
    "## 목차\n",
    "* 데이터 압축해제 및 확인하기\n",
    "* 파이프라인 구축하기\n",
    "* 모델 구조 설계하기\n",
    "* 각각의 모델 학습하기\n",
    "* 학습 결과 병합하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c98403",
   "metadata": {},
   "source": [
    "### STEP 1. 데이터 압축해제 및 확인하기\n",
    "---\n",
    "압축된 상태의 청크 파일을 압축해제하고, 파일이 어떻게 구성되어 있는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415e8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# simple 압축 파일 경로 지정\n",
    "compressed_file_path = os.getenv('HOME') + '/aiffel/Kaggle_Hackathon/data/simple_shuffle_data.zip'\n",
    "# 압축 해제를 진행할 경로 지정\n",
    "gz_file_path = os.getenv('HOME') + '/aiffel/Kaggle_Hackathon/data/compressed_simple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8fe52b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_k11.gz', 'train_k49.gz', 'train_k62.gz', 'train_k29.gz', 'train_k66.gz', 'train_k24.gz', 'train_k61.gz', 'train_k32.gz', 'train_k44.gz', 'train_k96.gz', 'train_k70.gz', 'train_k30.gz', 'train_k63.gz', 'train_k14.gz', 'train_k57.gz', 'train_k1.gz', 'train_k6.gz', 'train_k19.gz', 'train_k40.gz', 'train_k7.gz', 'train_k42.gz', 'train_k37.gz', 'train_k51.gz', 'train_k91.gz', 'train_k4.gz', 'train_k72.gz', 'train_k95.gz', 'train_k90.gz', 'train_k85.gz', 'train_k21.gz', 'train_k35.gz', 'train_k50.gz', 'train_k5.gz', 'train_k8.gz', 'train_k2.gz', 'train_k98.gz', 'train_k94.gz', 'train_k99.gz', 'train_k39.gz', 'train_k23.gz', 'train_k81.gz', 'train_k83.gz', 'train_k55.gz', 'train_k84.gz', 'train_k59.gz', 'train_k86.gz', 'train_k28.gz', 'train_k31.gz', 'train_k87.gz', 'train_k89.gz', 'train_k97.gz', 'train_k80.gz', 'train_k18.gz', 'train_k92.gz', 'train_k20.gz', 'train_k69.gz', 'train_k47.gz', 'train_k73.gz', 'train_k79.gz', 'train_k53.gz', 'train_k33.gz', 'train_k3.gz', 'train_k58.gz', 'train_k9.gz', 'train_k77.gz', 'train_k12.gz', 'train_k82.gz', 'train_k15.gz', 'train_k0.gz', 'train_k41.gz', 'train_k74.gz', 'train_k64.gz', 'train_k27.gz', 'train_k17.gz', 'train_k71.gz', 'train_k60.gz', 'train_k34.gz', 'train_k65.gz', 'train_k25.gz', 'train_k54.gz', 'train_k67.gz', 'train_k13.gz', 'train_k88.gz', 'train_k36.gz', 'train_k78.gz', 'train_k68.gz', 'train_k16.gz', 'train_k43.gz', 'train_k75.gz', 'train_k76.gz', 'train_k26.gz', 'train_k45.gz', 'train_k10.gz', 'train_k56.gz', 'train_k22.gz', 'train_k52.gz', 'train_k48.gz', 'train_k46.gz', 'train_k93.gz', 'train_k38.gz']\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# .zip 압축파일을 지정된 경로에 .gz형태의 chunk file들로 압축해제\n",
    "def extract_zip(current_file_path, extract_file_path):\n",
    "    with zipfile.ZipFile(current_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_file_path)\n",
    "        \n",
    "\n",
    "# 압축 해제 진행\n",
    "extract_zip(compressed_file_path, gz_file_path)\n",
    "\n",
    "# 정상적으로 압축해제가 진행됐는지 확인\n",
    "print(os.listdir(gz_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "740daf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_k6', 'train_k41', 'train_k55', 'train_k28', 'train_k78', 'train_k9', 'train_k43', 'train_k20', 'train_k61', 'train_k98', 'train_k18', 'train_k62', 'train_k27', 'train_k63', 'train_k38', 'train_k44', 'train_k16', 'train_k56', 'train_k97', 'train_k99', 'train_k33', 'train_k40', 'train_k60', 'train_k3', 'train_k47', 'train_k73', 'train_k49', 'train_k21', 'train_k12', 'train_k80', 'train_k66', 'train_k25', 'train_k74', 'train_k30', 'train_k71', 'train_k48', 'train_k11', 'train_k95', 'train_k65', 'train_k37', 'train_k31', 'train_k7', 'train_k2', 'train_k26', 'train_k8', 'train_k45', 'train_k83', 'train_k4', 'train_k94', 'train_k50', 'train_k77', 'train_k70', 'train_k86', 'train_k57', 'train_k29', 'train_k90', 'train_k93', 'train_k79', 'train_k67', 'train_k39', 'train_k32', 'train_k58', 'train_k92', 'train_k34', 'train_k68', 'train_k64', 'train_k36', 'train_k53', 'train_k10', 'train_k88', 'train_k51', 'train_k1', 'train_k59', 'train_k5', 'train_k76', 'train_k87', 'train_k23', 'train_k84', 'train_k72', 'train_k14', 'train_k19', 'train_k15', 'train_k91', '.ipynb_checkpoints', 'train_k24', 'train_k46', 'train_k52', 'train_k17', 'train_k85', 'train_k54', 'train_k75', 'train_k96', 'train_k81', 'train_k13', 'train_k89', 'train_k22', 'train_k42', 'train_k35', 'train_k82', 'train_k69', 'train_k0']\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# .gz 형태의 압축파일을 지정된 경로와 이름으로 압축해제\n",
    "def extract_gz(extract_file_path):\n",
    "    for i in range(100):\n",
    "        compressed_file_path = os.path.join(gz_file_path,f'train_k{i}.gz')\n",
    "        file_name = os.path.basename(compressed_file_path)\n",
    "        extract_path = os.path.join(extract_file_path, os.path.splitext(file_name)[0])\n",
    "\n",
    "        with gzip.open(compressed_file_path, 'rb') as f_in:\n",
    "            with open(extract_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "\n",
    "# 압축해제를 진행한 후 생성되는 파일들의 저장 경로 지정                \n",
    "file_path = os.getenv('HOME') + '/aiffel/Kaggle_Hackathon/data/simple'\n",
    "# 압축해제 진행\n",
    "extract_gz(file_path)\n",
    "\n",
    "# 정상적으로 압축해제가 진행됐는지 확인\n",
    "print(os.listdir(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f1b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             drawing    y\n",
      "0  [[[3, 4, 1, 114, 208, 248, 255, 157, 90, 28, 1...  194\n",
      "1  [[[50, 44, 29, 9, 0, 5, 25], [57, 55, 58, 70, ...  198\n",
      "2  [[[133, 156, 161, 167, 190, 204, 228, 251, 255...   16\n",
      "3  [[[0, 9, 25, 6, 1, 3, 10, 19, 35, 57, 89, 115,...  221\n",
      "4  [[[160, 246, 253, 254, 227, 190, 177, 167, 149...  173\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 0번 인덱스의 chunk file 경로 지정\n",
    "file_path = os.getenv('HOME') + '/aiffel/Kaggle_Hackathon/data/simple/train_k0'\n",
    "# chunk file 읽기\n",
    "data_df = pd.read_csv(file_path)\n",
    "# chunk file 데이터 확인\n",
    "print(data_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619d6a5",
   "metadata": {},
   "source": [
    "### STEP 2. 파이프라인 구축하기\n",
    "---\n",
    "압축해제된 데이터들 중 학습에 사용할 데이터를 핸들링하여 모델에 바로 입력할 수 있도록 준비합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26bc96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "import pickle\n",
    "\n",
    "# 데이터 파이프라인 함수 정의\n",
    "def data_pipeline(dir_path, pkl_path, file_idx):\n",
    "    # 데이터와 레이블을 담을 리스트 생성\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "\n",
    "    # 데이터 읽기\n",
    "    data = pd.read_csv(os.path.join(dir_path, f'train_k{file_idx}'))\n",
    "    # drawing 열을 리스트로 변환\n",
    "    drawing = data['drawing'].apply(ast.literal_eval).tolist()\n",
    "    \n",
    "    # 레이블 이름이 담긴 파일 읽기\n",
    "    with open(pkl_path, 'rb') as f:\n",
    "        label_names = pickle.load(f)\n",
    "\n",
    "    for draw_idx, label_idx in zip(drawing, data['y']):\n",
    "        # 256X256 크기의 흰색 이미지 생성\n",
    "        img = Image.new('L', (256, 256), color='white')\n",
    "        # 배경으로 지정\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # 배열의 각 요소를 만들어 놓은 이미지에 그리기\n",
    "        for stroke in draw_idx:\n",
    "            for i in range(len(stroke[0]) - 1):\n",
    "                draw.line([stroke[0][i], stroke[1][i], stroke[0][i+1], stroke[1][i+1]], fill='black', width=3)\n",
    "\n",
    "        # 메모리 절감을 위해 이미지의 크기를 28X28로 변환\n",
    "        img = img.resize((28, 28))\n",
    "\n",
    "        # 이미지를 배열의 형태로 변환\n",
    "        image_array = img_to_array(img)\n",
    "        # 변환된 데이터를 모델에 입력하기 용이하도록 텐서로 변환\n",
    "        image_array = preprocess_input(image_array)\n",
    "\n",
    "        # 학습 데이터 리스트에 추가\n",
    "        data_list.append(tf.convert_to_tensor(image_array, dtype=tf.float32))\n",
    "        # 학습 레이블 리스트에 추가\n",
    "        label_list.append(label_names[label_idx])\n",
    "\n",
    "    # DataFrame 생성\n",
    "    data_df = pd.DataFrame({'img': data_list, 'label': label_list})\n",
    "\n",
    "    return data_df\n",
    "\n",
    "# 부모 디렉터리 경로 지정\n",
    "root_path = os.getenv('HOME') + '/aiffel/Kaggle_Hackathon/data'\n",
    "# chunk file이 존재하는 경로 지정\n",
    "dir_path = os.path.join(root_path, 'simple')\n",
    "# 레이블 이름 파일이 존재하는 경로 지정\n",
    "pkl_path = os.path.join(root_path, 'label_names.pkl')\n",
    "\n",
    "# 파이프라인 함수 호출\n",
    "data_df = data_pipeline(dir_path, pkl_path, 50)\n",
    "\n",
    "# 만들어진 데이터프레임 확인\n",
    "print(data_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
