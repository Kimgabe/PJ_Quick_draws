{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f002929",
   "metadata": {},
   "source": [
    "# 0207-Data preprocessing\n",
    "- 대용량 데이터를 효율적으로 처리할수 있는 파이프라인 구축 및 데이터 전처리 시도 1\n",
    "\n",
    "\n",
    "# try 1 : pyconkr 2019 대용량 데이터 핸들링 관련 자료 참고\n",
    "- 데이터 타입 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16a290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d0112",
   "metadata": {},
   "source": [
    "## 데이터 파일명 수정\n",
    "- train_simplified 데이터 폴더명은 각 레이블로 되어있는데, \\_없이 공백으로 파일명이 작성되어있음\n",
    "    - e.g. alarm clock.csv\n",
    "- 이 부분 전처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6465e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "# Function to extract and rename folders\n",
    "def extract_and_rename_folders(zip_file_path, extract_path):\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    \n",
    "    # Get a list of folders in the extracted path\n",
    "    extracted_folders = os.listdir(extract_path)\n",
    "    \n",
    "    # Iterate over the extracted folders\n",
    "    for folder_name in extracted_folders:\n",
    "        old_path = os.path.join(extract_path, folder_name)\n",
    "        # Check if the folder name contains spaces\n",
    "        if ' ' in folder_name:\n",
    "            # New folder name with spaces removed\n",
    "            new_folder_name = folder_name.replace(' ', '_')\n",
    "            new_path = os.path.join(extract_path, new_folder_name)\n",
    "            # Rename the folder\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed folder '{folder_name}' to '{new_folder_name}'\")\n",
    "        else:\n",
    "            print(f\"No spaces found in folder '{folder_name}', skipping renaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac619745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed folder 'tennis racquet.csv' to 'tennis_racquet.csv'\n",
      "No spaces found in folder 'spreadsheet.csv', skipping renaming\n",
      "No spaces found in folder 'scissors.csv', skipping renaming\n",
      "No spaces found in folder 'belt.csv', skipping renaming\n",
      "No spaces found in folder 'whale.csv', skipping renaming\n",
      "No spaces found in folder 'table.csv', skipping renaming\n",
      "No spaces found in folder 'moustache.csv', skipping renaming\n",
      "No spaces found in folder 'envelope.csv', skipping renaming\n",
      "Renamed folder 'washing machine.csv' to 'washing_machine.csv'\n",
      "No spaces found in folder 'camel.csv', skipping renaming\n",
      "No spaces found in folder 'lighthouse.csv', skipping renaming\n",
      "No spaces found in folder 'scorpion.csv', skipping renaming\n",
      "No spaces found in folder 'pig.csv', skipping renaming\n",
      "No spaces found in folder 'snake.csv', skipping renaming\n",
      "No spaces found in folder 'stitches.csv', skipping renaming\n",
      "No spaces found in folder 'trombone.csv', skipping renaming\n",
      "No spaces found in folder 'cup.csv', skipping renaming\n",
      "Renamed folder 'frying pan.csv' to 'frying_pan.csv'\n",
      "No spaces found in folder 'rake.csv', skipping renaming\n",
      "Renamed folder 'police car.csv' to 'police_car.csv'\n",
      "No spaces found in folder 'rollerskates.csv', skipping renaming\n",
      "No spaces found in folder 'pond.csv', skipping renaming\n",
      "No spaces found in folder 'peas.csv', skipping renaming\n",
      "No spaces found in folder 'rhinoceros.csv', skipping renaming\n",
      "No spaces found in folder 'banana.csv', skipping renaming\n",
      "No spaces found in folder 'dog.csv', skipping renaming\n",
      "No spaces found in folder 'microphone.csv', skipping renaming\n",
      "No spaces found in folder 'headphones.csv', skipping renaming\n",
      "No spaces found in folder 'harp.csv', skipping renaming\n",
      "No spaces found in folder 'ambulance.csv', skipping renaming\n",
      "No spaces found in folder 'garden.csv', skipping renaming\n",
      "No spaces found in folder 'streetlight.csv', skipping renaming\n",
      "Renamed folder 'palm tree.csv' to 'palm_tree.csv'\n",
      "No spaces found in folder 'kangaroo.csv', skipping renaming\n",
      "No spaces found in folder 'hand.csv', skipping renaming\n",
      "No spaces found in folder 'pillow.csv', skipping renaming\n",
      "No spaces found in folder 'pineapple.csv', skipping renaming\n",
      "No spaces found in folder 'sock.csv', skipping renaming\n",
      "No spaces found in folder 'marker.csv', skipping renaming\n",
      "No spaces found in folder 'mushroom.csv', skipping renaming\n",
      "No spaces found in folder 'parachute.csv', skipping renaming\n",
      "No spaces found in folder 'cat.csv', skipping renaming\n",
      "No spaces found in folder 'sink.csv', skipping renaming\n",
      "No spaces found in folder 'candle.csv', skipping renaming\n",
      "Renamed folder 'picture frame.csv' to 'picture_frame.csv'\n",
      "No spaces found in folder 'foot.csv', skipping renaming\n",
      "Renamed folder 'school bus.csv' to 'school_bus.csv'\n",
      "No spaces found in folder 'stove.csv', skipping renaming\n",
      "No spaces found in folder 'grapes.csv', skipping renaming\n",
      "Renamed folder 'fire hydrant.csv' to 'fire_hydrant.csv'\n",
      "No spaces found in folder 'crocodile.csv', skipping renaming\n",
      "No spaces found in folder 'sun.csv', skipping renaming\n",
      "No spaces found in folder 'cello.csv', skipping renaming\n",
      "No spaces found in folder 'oven.csv', skipping renaming\n",
      "No spaces found in folder 'watermelon.csv', skipping renaming\n",
      "No spaces found in folder 'mouth.csv', skipping renaming\n",
      "No spaces found in folder 'anvil.csv', skipping renaming\n",
      "No spaces found in folder 'sailboat.csv', skipping renaming\n",
      "No spaces found in folder 'car.csv', skipping renaming\n",
      "No spaces found in folder 'truck.csv', skipping renaming\n",
      "No spaces found in folder 'ant.csv', skipping renaming\n",
      "No spaces found in folder 'ear.csv', skipping renaming\n",
      "No spaces found in folder 'windmill.csv', skipping renaming\n",
      "No spaces found in folder 'television.csv', skipping renaming\n",
      "No spaces found in folder 'canoe.csv', skipping renaming\n",
      "Renamed folder 'hockey puck.csv' to 'hockey_puck.csv'\n",
      "No spaces found in folder 'submarine.csv', skipping renaming\n",
      "No spaces found in folder 'apple.csv', skipping renaming\n",
      "No spaces found in folder 'triangle.csv', skipping renaming\n",
      "No spaces found in folder 'pool.csv', skipping renaming\n",
      "No spaces found in folder 'umbrella.csv', skipping renaming\n",
      "No spaces found in folder 'stethoscope.csv', skipping renaming\n",
      "Renamed folder 'wine bottle.csv' to 'wine_bottle.csv'\n",
      "Renamed folder 'golf club.csv' to 'golf_club.csv'\n",
      "No spaces found in folder 'mountain.csv', skipping renaming\n",
      "No spaces found in folder 'bowtie.csv', skipping renaming\n",
      "No spaces found in folder 'pliers.csv', skipping renaming\n",
      "Renamed folder 'hot tub.csv' to 'hot_tub.csv'\n",
      "No spaces found in folder 'octopus.csv', skipping renaming\n",
      "No spaces found in folder 'octagon.csv', skipping renaming\n",
      "Renamed folder 'power outlet.csv' to 'power_outlet.csv'\n",
      "No spaces found in folder 'helicopter.csv', skipping renaming\n",
      "No spaces found in folder 'bus.csv', skipping renaming\n",
      "No spaces found in folder 'tooth.csv', skipping renaming\n",
      "No spaces found in folder 'eraser.csv', skipping renaming\n",
      "No spaces found in folder 'van.csv', skipping renaming\n",
      "No spaces found in folder 'dolphin.csv', skipping renaming\n",
      "Renamed folder 'cell phone.csv' to 'cell_phone.csv'\n",
      "No spaces found in folder 'skateboard.csv', skipping renaming\n",
      "No spaces found in folder 'cookie.csv', skipping renaming\n",
      "No spaces found in folder 'eyeglasses.csv', skipping renaming\n",
      "No spaces found in folder 'elbow.csv', skipping renaming\n",
      "No spaces found in folder 'ocean.csv', skipping renaming\n",
      "No spaces found in folder 'snowman.csv', skipping renaming\n",
      "No spaces found in folder 'potato.csv', skipping renaming\n",
      "No spaces found in folder 'bear.csv', skipping renaming\n",
      "Renamed folder 'wine glass.csv' to 'wine_glass.csv'\n",
      "No spaces found in folder 'matches.csv', skipping renaming\n",
      "No spaces found in folder 'rabbit.csv', skipping renaming\n",
      "Renamed folder 'The Eiffel Tower.csv' to 'The_Eiffel_Tower.csv'\n",
      "No spaces found in folder 'sandwich.csv', skipping renaming\n",
      "No spaces found in folder 'bicycle.csv', skipping renaming\n",
      "Renamed folder 'coffee cup.csv' to 'coffee_cup.csv'\n",
      "No spaces found in folder 'jail.csv', skipping renaming\n",
      "No spaces found in folder 'sheep.csv', skipping renaming\n",
      "No spaces found in folder 'camouflage.csv', skipping renaming\n",
      "No spaces found in folder 'line.csv', skipping renaming\n",
      "Renamed folder 'house plant.csv' to 'house_plant.csv'\n",
      "No spaces found in folder 'saw.csv', skipping renaming\n",
      "No spaces found in folder 'wristwatch.csv', skipping renaming\n",
      "No spaces found in folder 'eye.csv', skipping renaming\n",
      "No spaces found in folder 'stairs.csv', skipping renaming\n",
      "No spaces found in folder 'snowflake.csv', skipping renaming\n",
      "Renamed folder 'string bean.csv' to 'string_bean.csv'\n",
      "No spaces found in folder 'bed.csv', skipping renaming\n",
      "Renamed folder 'see saw.csv' to 'see_saw.csv'\n",
      "No spaces found in folder 'bread.csv', skipping renaming\n",
      "No spaces found in folder 'lantern.csv', skipping renaming\n",
      "No spaces found in folder 'map.csv', skipping renaming\n",
      "No spaces found in folder 'mosquito.csv', skipping renaming\n",
      "No spaces found in folder 'lipstick.csv', skipping renaming\n",
      "No spaces found in folder 't-shirt.csv', skipping renaming\n",
      "No spaces found in folder 'bracelet.csv', skipping renaming\n",
      "No spaces found in folder 'hexagon.csv', skipping renaming\n",
      "Renamed folder 'ice cream.csv' to 'ice_cream.csv'\n",
      "No spaces found in folder 'crayon.csv', skipping renaming\n",
      "No spaces found in folder 'tornado.csv', skipping renaming\n",
      "No spaces found in folder 'diamond.csv', skipping renaming\n",
      "Renamed folder 'sea turtle.csv' to 'sea_turtle.csv'\n",
      "No spaces found in folder 'panda.csv', skipping renaming\n",
      "Renamed folder 'traffic light.csv' to 'traffic_light.csv'\n",
      "No spaces found in folder 'binoculars.csv', skipping renaming\n",
      "Renamed folder 'hot dog.csv' to 'hot_dog.csv'\n",
      "No spaces found in folder 'underwear.csv', skipping renaming\n",
      "Renamed folder 'soccer ball.csv' to 'soccer_ball.csv'\n",
      "No spaces found in folder 'passport.csv', skipping renaming\n",
      "Renamed folder 'baseball bat.csv' to 'baseball_bat.csv'\n",
      "No spaces found in folder 'owl.csv', skipping renaming\n",
      "No spaces found in folder 'fan.csv', skipping renaming\n",
      "No spaces found in folder 'broom.csv', skipping renaming\n",
      "No spaces found in folder 'horse.csv', skipping renaming\n",
      "No spaces found in folder 'sword.csv', skipping renaming\n",
      "No spaces found in folder 'bridge.csv', skipping renaming\n",
      "No spaces found in folder 'tree.csv', skipping renaming\n",
      "No spaces found in folder 'rainbow.csv', skipping renaming\n",
      "No spaces found in folder 'butterfly.csv', skipping renaming\n",
      "No spaces found in folder 'strawberry.csv', skipping renaming\n",
      "No spaces found in folder 'nail.csv', skipping renaming\n",
      "No spaces found in folder 'popsicle.csv', skipping renaming\n",
      "No spaces found in folder 'zigzag.csv', skipping renaming\n",
      "No spaces found in folder 'moon.csv', skipping renaming\n",
      "No spaces found in folder 'grass.csv', skipping renaming\n",
      "No spaces found in folder 'backpack.csv', skipping renaming\n",
      "No spaces found in folder 'boomerang.csv', skipping renaming\n",
      "No spaces found in folder 'hospital.csv', skipping renaming\n",
      "No spaces found in folder 'fence.csv', skipping renaming\n",
      "No spaces found in folder 'wheel.csv', skipping renaming\n",
      "Renamed folder 'cruise ship.csv' to 'cruise_ship.csv'\n",
      "No spaces found in folder 'suitcase.csv', skipping renaming\n",
      "No spaces found in folder 'fork.csv', skipping renaming\n",
      "No spaces found in folder 'mermaid.csv', skipping renaming\n",
      "No spaces found in folder 'clarinet.csv', skipping renaming\n",
      "No spaces found in folder 'duck.csv', skipping renaming\n",
      "No spaces found in folder 'toilet.csv', skipping renaming\n",
      "No spaces found in folder 'knee.csv', skipping renaming\n",
      "No spaces found in folder 'chandelier.csv', skipping renaming\n",
      "No spaces found in folder 'microwave.csv', skipping renaming\n",
      "No spaces found in folder 'chair.csv', skipping renaming\n",
      "No spaces found in folder 'snorkel.csv', skipping renaming\n",
      "No spaces found in folder 'guitar.csv', skipping renaming\n",
      "No spaces found in folder 'blueberry.csv', skipping renaming\n",
      "No spaces found in folder 'key.csv', skipping renaming\n",
      "Renamed folder 'garden hose.csv' to 'garden_hose.csv'\n",
      "No spaces found in folder 'rain.csv', skipping renaming\n",
      "No spaces found in folder 'hedgehog.csv', skipping renaming\n",
      "No spaces found in folder 'shark.csv', skipping renaming\n",
      "No spaces found in folder 'fireplace.csv', skipping renaming\n",
      "No spaces found in folder 'laptop.csv', skipping renaming\n",
      "Renamed folder 'roller coaster.csv' to 'roller_coaster.csv'\n",
      "No spaces found in folder 'castle.csv', skipping renaming\n",
      "No spaces found in folder 'flashlight.csv', skipping renaming\n",
      "No spaces found in folder 'giraffe.csv', skipping renaming\n",
      "No spaces found in folder 'bathtub.csv', skipping renaming\n",
      "No spaces found in folder 'vase.csv', skipping renaming\n",
      "No spaces found in folder 'raccoon.csv', skipping renaming\n",
      "No spaces found in folder 'nose.csv', skipping renaming\n",
      "No spaces found in folder 'dresser.csv', skipping renaming\n",
      "No spaces found in folder 'squiggle.csv', skipping renaming\n",
      "No spaces found in folder 'skull.csv', skipping renaming\n",
      "Renamed folder 'remote control.csv' to 'remote_control.csv'\n",
      "No spaces found in folder 'peanut.csv', skipping renaming\n",
      "No spaces found in folder 'angel.csv', skipping renaming\n",
      "No spaces found in folder 'beach.csv', skipping renaming\n",
      "No spaces found in folder 'pencil.csv', skipping renaming\n",
      "No spaces found in folder 'pear.csv', skipping renaming\n",
      "Renamed folder 'pickup truck.csv' to 'pickup_truck.csv'\n",
      "No spaces found in folder 'face.csv', skipping renaming\n",
      "No spaces found in folder 'cooler.csv', skipping renaming\n",
      "No spaces found in folder 'brain.csv', skipping renaming\n",
      "No spaces found in folder 'monkey.csv', skipping renaming\n",
      "No spaces found in folder 'circle.csv', skipping renaming\n",
      "No spaces found in folder 'purse.csv', skipping renaming\n",
      "No spaces found in folder 'computer.csv', skipping renaming\n",
      "No spaces found in folder 'violin.csv', skipping renaming\n",
      "No spaces found in folder 'firetruck.csv', skipping renaming\n",
      "No spaces found in folder 'carrot.csv', skipping renaming\n",
      "No spaces found in folder 'basketball.csv', skipping renaming\n",
      "No spaces found in folder 'screwdriver.csv', skipping renaming\n",
      "No spaces found in folder 'airplane.csv', skipping renaming\n",
      "No spaces found in folder 'flower.csv', skipping renaming\n",
      "No spaces found in folder 'pants.csv', skipping renaming\n",
      "No spaces found in folder 'penguin.csv', skipping renaming\n",
      "No spaces found in folder 'elephant.csv', skipping renaming\n",
      "No spaces found in folder 'drill.csv', skipping renaming\n",
      "No spaces found in folder 'tent.csv', skipping renaming\n",
      "No spaces found in folder 'toothpaste.csv', skipping renaming\n",
      "No spaces found in folder 'waterslide.csv', skipping renaming\n",
      "No spaces found in folder 'finger.csv', skipping renaming\n",
      "No spaces found in folder 'arm.csv', skipping renaming\n",
      "No spaces found in folder 'squirrel.csv', skipping renaming\n",
      "No spaces found in folder 'steak.csv', skipping renaming\n",
      "No spaces found in folder 'bandage.csv', skipping renaming\n",
      "No spaces found in folder 'shorts.csv', skipping renaming\n",
      "No spaces found in folder 'teddy-bear.csv', skipping renaming\n",
      "Renamed folder 'paint can.csv' to 'paint_can.csv'\n",
      "Renamed folder 'animal migration.csv' to 'animal_migration.csv'\n",
      "No spaces found in folder 'camera.csv', skipping renaming\n",
      "No spaces found in folder 'helmet.csv', skipping renaming\n",
      "Renamed folder 'paper clip.csv' to 'paper_clip.csv'\n",
      "No spaces found in folder 'barn.csv', skipping renaming\n",
      "Renamed folder 'alarm clock.csv' to 'alarm_clock.csv'\n",
      "No spaces found in folder 'dumbbell.csv', skipping renaming\n",
      "No spaces found in folder 'beard.csv', skipping renaming\n",
      "Renamed folder 'swing set.csv' to 'swing_set.csv'\n",
      "No spaces found in folder 'mouse.csv', skipping renaming\n",
      "Renamed folder 'The Great Wall of China.csv' to 'The_Great_Wall_of_China.csv'\n",
      "No spaces found in folder 'toaster.csv', skipping renaming\n",
      "No spaces found in folder 'crown.csv', skipping renaming\n",
      "No spaces found in folder 'piano.csv', skipping renaming\n",
      "No spaces found in folder 'zebra.csv', skipping renaming\n",
      "No spaces found in folder 'star.csv', skipping renaming\n",
      "No spaces found in folder 'hurricane.csv', skipping renaming\n",
      "No spaces found in folder 'leaf.csv', skipping renaming\n",
      "No spaces found in folder 'baseball.csv', skipping renaming\n",
      "No spaces found in folder 'sweater.csv', skipping renaming\n",
      "No spaces found in folder 'dragon.csv', skipping renaming\n",
      "No spaces found in folder 'snail.csv', skipping renaming\n",
      "No spaces found in folder 'tractor.csv', skipping renaming\n",
      "No spaces found in folder 'cloud.csv', skipping renaming\n",
      "No spaces found in folder 'door.csv', skipping renaming\n",
      "No spaces found in folder 'donut.csv', skipping renaming\n",
      "No spaces found in folder 'motorbike.csv', skipping renaming\n",
      "No spaces found in folder 'teapot.csv', skipping renaming\n",
      "No spaces found in folder 'feather.csv', skipping renaming\n",
      "No spaces found in folder 'bucket.csv', skipping renaming\n",
      "Renamed folder 'light bulb.csv' to 'light_bulb.csv'\n",
      "No spaces found in folder 'drums.csv', skipping renaming\n",
      "No spaces found in folder 'church.csv', skipping renaming\n",
      "No spaces found in folder 'ladder.csv', skipping renaming\n",
      "No spaces found in folder 'cactus.csv', skipping renaming\n",
      "No spaces found in folder 'couch.csv', skipping renaming\n",
      "No spaces found in folder 'bird.csv', skipping renaming\n",
      "No spaces found in folder 'flamingo.csv', skipping renaming\n",
      "Renamed folder 'smiley face.csv' to 'smiley_face.csv'\n",
      "No spaces found in folder 'hamburger.csv', skipping renaming\n",
      "Renamed folder 'stop sign.csv' to 'stop_sign.csv'\n",
      "No spaces found in folder 'campfire.csv', skipping renaming\n",
      "No spaces found in folder 'frog.csv', skipping renaming\n",
      "No spaces found in folder 'bat.csv', skipping renaming\n",
      "No spaces found in folder 'calculator.csv', skipping renaming\n",
      "No spaces found in folder 'cannon.csv', skipping renaming\n",
      "No spaces found in folder 'trumpet.csv', skipping renaming\n",
      "No spaces found in folder 'radio.csv', skipping renaming\n",
      "No spaces found in folder 'tiger.csv', skipping renaming\n",
      "No spaces found in folder 'bee.csv', skipping renaming\n",
      "No spaces found in folder 'lollipop.csv', skipping renaming\n",
      "Renamed folder 'flip flops.csv' to 'flip_flops.csv'\n",
      "No spaces found in folder 'bench.csv', skipping renaming\n",
      "No spaces found in folder 'telephone.csv', skipping renaming\n",
      "No spaces found in folder 'toe.csv', skipping renaming\n",
      "No spaces found in folder 'clock.csv', skipping renaming\n",
      "No spaces found in folder 'spider.csv', skipping renaming\n",
      "No spaces found in folder 'shovel.csv', skipping renaming\n",
      "No spaces found in folder 'calendar.csv', skipping renaming\n",
      "Renamed folder 'floor lamp.csv' to 'floor_lamp.csv'\n",
      "No spaces found in folder 'leg.csv', skipping renaming\n",
      "No spaces found in folder 'spoon.csv', skipping renaming\n",
      "No spaces found in folder 'onion.csv', skipping renaming\n",
      "No spaces found in folder 'pizza.csv', skipping renaming\n",
      "No spaces found in folder 'shoe.csv', skipping renaming\n",
      "No spaces found in folder 'yoga.csv', skipping renaming\n",
      "No spaces found in folder 'axe.csv', skipping renaming\n",
      "Renamed folder 'ceiling fan.csv' to 'ceiling_fan.csv'\n",
      "No spaces found in folder 'necklace.csv', skipping renaming\n",
      "No spaces found in folder 'crab.csv', skipping renaming\n",
      "No spaces found in folder 'goatee.csv', skipping renaming\n",
      "No spaces found in folder 'postcard.csv', skipping renaming\n",
      "No spaces found in folder 'mailbox.csv', skipping renaming\n",
      "No spaces found in folder 'bush.csv', skipping renaming\n",
      "No spaces found in folder 'speedboat.csv', skipping renaming\n",
      "No spaces found in folder 'asparagus.csv', skipping renaming\n",
      "No spaces found in folder 'broccoli.csv', skipping renaming\n",
      "No spaces found in folder 'train.csv', skipping renaming\n",
      "Renamed folder 'The Mona Lisa.csv' to 'The_Mona_Lisa.csv'\n",
      "No spaces found in folder 'saxophone.csv', skipping renaming\n",
      "No spaces found in folder 'lightning.csv', skipping renaming\n",
      "No spaces found in folder 'house.csv', skipping renaming\n",
      "No spaces found in folder 'cake.csv', skipping renaming\n",
      "No spaces found in folder 'compass.csv', skipping renaming\n",
      "Renamed folder 'diving board.csv' to 'diving_board.csv'\n",
      "No spaces found in folder 'parrot.csv', skipping renaming\n",
      "No spaces found in folder 'paintbrush.csv', skipping renaming\n",
      "Renamed folder 'hot air balloon.csv' to 'hot_air_balloon.csv'\n",
      "No spaces found in folder 'mug.csv', skipping renaming\n",
      "No spaces found in folder 'bottlecap.csv', skipping renaming\n",
      "No spaces found in folder 'square.csv', skipping renaming\n",
      "No spaces found in folder 'swan.csv', skipping renaming\n",
      "No spaces found in folder 'basket.csv', skipping renaming\n",
      "No spaces found in folder 'keyboard.csv', skipping renaming\n",
      "No spaces found in folder 'megaphone.csv', skipping renaming\n",
      "No spaces found in folder 'hammer.csv', skipping renaming\n",
      "No spaces found in folder 'lobster.csv', skipping renaming\n",
      "No spaces found in folder 'blackberry.csv', skipping renaming\n",
      "Renamed folder 'birthday cake.csv' to 'birthday_cake.csv'\n",
      "No spaces found in folder 'toothbrush.csv', skipping renaming\n",
      "No spaces found in folder 'lion.csv', skipping renaming\n",
      "No spaces found in folder 'river.csv', skipping renaming\n",
      "Renamed folder 'hockey stick.csv' to 'hockey_stick.csv'\n",
      "Renamed folder 'flying saucer.csv' to 'flying_saucer.csv'\n",
      "No spaces found in folder 'skyscraper.csv', skipping renaming\n",
      "No spaces found in folder 'book.csv', skipping renaming\n",
      "No spaces found in folder 'jacket.csv', skipping renaming\n",
      "No spaces found in folder 'hat.csv', skipping renaming\n",
      "No spaces found in folder 'dishwasher.csv', skipping renaming\n",
      "Renamed folder 'sleeping bag.csv' to 'sleeping_bag.csv'\n",
      "No spaces found in folder 'hourglass.csv', skipping renaming\n",
      "No spaces found in folder 'cow.csv', skipping renaming\n",
      "No spaces found in folder 'fish.csv', skipping renaming\n",
      "No spaces found in folder 'bulldozer.csv', skipping renaming\n",
      "No spaces found in folder 'stereo.csv', skipping renaming\n"
     ]
    }
   ],
   "source": [
    "# Path to the train_simplified.zip file\n",
    "train_simplified_zip_path = '../data/train_simplified.zip'\n",
    "# Directory where you want to extract the zip file\n",
    "extract_path = '../data/train_simplified'\n",
    "\n",
    "# Extract and rename folders in the zip file\n",
    "extract_and_rename_folders(train_simplified_zip_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db5d7d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "상위폴더 주소 /aiffel/aiffel/Sidethon\n"
     ]
    }
   ],
   "source": [
    "# 현재 작업 디렉토리의 주소를 얻음\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# 상위 폴더의 주소\n",
    "ROOT_PATH = os.path.dirname(current_directory)\n",
    "\n",
    "print('상위폴더 주소', ROOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "798cfe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/Sidethon/data/train_simplified\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = ROOT_PATH + '/data/train_simplified'\n",
    "print(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15674a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/Sidethon/data/test_simplified.csv\n"
     ]
    }
   ],
   "source": [
    "TEST_PATH = ROOT_PATH + '/data/test_simplified.csv'\n",
    "print(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33c9a89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1\n",
      "test: 1\n"
     ]
    }
   ],
   "source": [
    "train_filenames = tf.io.gfile.glob(TRAIN_PATH)\n",
    "test_filename = tf.io.gfile.glob(TEST_PATH)\n",
    "\n",
    "train_count = len(train_filenames)\n",
    "test_count = len(test_filename)\n",
    "print(\"train:\", train_count)\n",
    "print(\"test:\", test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca2fadbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/aiffel/aiffel/Sidethon/data/train_simplified/snowman.csv', '/aiffel/aiffel/Sidethon/data/train_simplified/potato.csv', '/aiffel/aiffel/Sidethon/data/train_simplified/bear.csv', '/aiffel/aiffel/Sidethon/data/train_simplified/matches.csv']\n",
      "['/aiffel/aiffel/Sidethon/data/test_simplified.csv']\n"
     ]
    }
   ],
   "source": [
    "print(train_filenames[:4])\n",
    "print(test_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee69f6",
   "metadata": {},
   "source": [
    "- 각 csv 파일명이 변수에 담겨있음\n",
    "- 각 파일에서 필요한 컬럼 정보만 추출해서 데이터프레임에 담고 이 데이터프레임으로 데이터셋을 만든다\n",
    "    - 이 과정에서 각 데이터에 적합한 dtype을 찾아서 해당 타입으로 변환하는 방법을 시도해본다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff27748",
   "metadata": {},
   "source": [
    "## check_dtypes() 함수\n",
    "- 각 컬럼 내 데이터의 최소, 최대 범위를 계산해 적절한 Data Type을 찾아내는 함수\n",
    "- 각 컬럼의 데이터 형식을 모를때 자동으로 체크하여 사용하고 데이터를 불러올때 체크된 데이터 형식으로 데이터를 불러옴\n",
    "- 형식을 지정하지 않으면 가장 메모리를 많이 차지하는 방식으로 데이터를 불러옴 -> 형식을 지정해서 데이터를 불러오면 메모리를 줄여서 큰 데이터도 불러올수 있음\n",
    "    - 데이터 형식 크기 비교: Object > complex > datetime64, float64, int64 > float32, int32 > …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1dc06504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dtypes(file_path):\n",
    "    print(file_path)\n",
    "    tmp = pd.read_csv(file_path, nrows=0)\n",
    "    col_dtypes = {}\n",
    "    print(tmp.columns)\n",
    "    for col in tmp.columns:\n",
    "        df = pd.read_csv(file_path, usecols=[col])\n",
    "        dtype = df[col].dtype\n",
    "        \n",
    "        if dtype == 'int' or dtype == 'float':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "        elif dtype == 'object':\n",
    "            n_unique = df[col].nunique()\n",
    "            threshold = n_unique / df.shape[0]\n",
    "            \n",
    "        if dtype == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                col_dtype = 'int8'\n",
    "            elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:  \n",
    "                col_dtype = 'uint8'  \n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                col_dtype = 'int16'\n",
    "            elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:  \n",
    "                col_dtype = 'uint16'  \n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                col_dtype = 'int32'\n",
    "            elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:  \n",
    "                col_dtype = 'uint32' \n",
    "            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                col_dtype = 'int64'\n",
    "            elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:  \n",
    "                col_dtype = 'uint64'  \n",
    "            \n",
    "        elif dtype == 'float':\n",
    "            # ERROR occured When using float32 in feather, parquet\n",
    "#             if c_min > np.finfo(np.float18).min and c_max < np.finfo(np.float18).max:\n",
    "#                 col_dtype = 'float18'\n",
    "            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                col_dtype = 'float32'\n",
    "            else:\n",
    "                col_dtype = 'float64'\n",
    "                \n",
    "        elif dtype == 'object':\n",
    "            if threshold > 0.7 :\n",
    "                col_dtype = 'object'\n",
    "            else:\n",
    "                col_dtype = 'category'\n",
    "        \n",
    "        col_dtypes[col] = col_dtype\n",
    "    \n",
    "    return col_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50dbc24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/Sidethon/data/train_simplified/snowman.csv\n",
      "Index(['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'countrycode': 'category',\n",
       " 'drawing': 'object',\n",
       " 'key_id': 'int64',\n",
       " 'recognized': 'int64',\n",
       " 'timestamp': 'object',\n",
       " 'word': 'category'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 : train데이터 경로 한개를 가져와 테스트\n",
    "col_dtypes = check_dtypes(train_filenames[0])\n",
    "col_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5accc",
   "metadata": {},
   "source": [
    "1. 원하는 컬럼만 추출 - key_id, drawing, word(label)\n",
    "2. key_id int32형식으로 변환\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# try 2 : tf.data의 AUTOTUNE, prefetch 기능 확인후 적용 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a808af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess one csv file\n",
    "# # Function to preprocess data and create a TensorFlow dataset\n",
    "# def preprocess_data(file_path, columns_to_use, batch_size):\n",
    "#     # Read CSV file with selected columns\n",
    "#     df = pd.read_csv(file_path, usecols=columns_to_use)\n",
    "    \n",
    "#     # Convert 'object' columns to 'category' data type\n",
    "#     for col in df.select_dtypes(include=['object']).columns:\n",
    "#         df[col] = df[col].astype('category')\n",
    "        \n",
    "#     # Convert 'int64' columns to 'int32' data type\n",
    "#     for col in df.select_dtypes(include=['int64']).columns:\n",
    "#         df[col] = df[col].astype('int32')\n",
    "    \n",
    "#     # Convert DataFrame to TensorFlow dataset\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "    \n",
    "#     # Shuffle and batch the dataset\n",
    "#     dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "    \n",
    "#     # Prefetch and autotune for performance optimization\n",
    "#     dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06ff509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data from multiple CSV files in a folder and create a TensorFlow dataset\n",
    "def preprocess_data_from_folder(file_paths, columns_to_use, batch_size):\n",
    "        # Initialize an empty list to store datasets for each CSV file\n",
    "    datasets = []\n",
    "    \n",
    "    # Iterate over each CSV file\n",
    "    for file_path in file_paths:\n",
    "        # Read CSV file with selected columns\n",
    "        df = pd.read_csv(file_path, usecols=columns_to_use)\n",
    "        \n",
    "        # Convert 'object' columns to 'category' data type\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            df[col] = df[col].astype('category')\n",
    "            \n",
    "        # Convert 'int64' columns to 'int32' data type\n",
    "        for col in df.select_dtypes(include=['int64']).columns:\n",
    "            df[col] = df[col].astype('int32')\n",
    "        \n",
    "        # Convert DataFrame to TensorFlow dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "        \n",
    "        # Append the dataset to the list\n",
    "        datasets.append(dataset)\n",
    "    \n",
    "    # Concatenate datasets from all CSV files into one dataset\n",
    "    combined_dataset = tf.data.experimental.sample_from_datasets(datasets)\n",
    "    \n",
    "    # Shuffle and batch the combined dataset\n",
    "    combined_dataset = combined_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "    \n",
    "    # Prefetch and autotune for performance optimization\n",
    "    combined_dataset = combined_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9628e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folder path, columns to use, and batch size\n",
    "# folder_path = '/aiffel/aiffel/Sidethon/data/train_simplified/'\n",
    "columns_to_use = ['key_id', 'drawing', 'word']\n",
    "batch_size = 32\n",
    "\n",
    "# Preprocess data from folder and create TensorFlow dataset\n",
    "train_dataset = preprocess_data_from_folder(train_filenames, columns_to_use, batch_size)\n",
    "\n",
    "# Iterate over the dataset (for demonstration purposes)\n",
    "for batch in train_dataset.take(1):\n",
    "    # Accessing a batch of data\n",
    "    print(\"Batch:\", batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43928e3",
   "metadata": {},
   "source": [
    "커널이 죽는 문제 발생!\n",
    "\n",
    "---\n",
    "\n",
    "# try 3 : tf.data 방식 변경\n",
    "\n",
    "아래 방식으로 변경\n",
    "- `tf.data.Dataset.list_files` : to create a dataset of file paths in the folder\n",
    "- `tf.data.experimental.CsvDataset` : to read and parse CSV records directly within the TensorFlow pipeline\n",
    "- `tf.cast`: to optimize data types within the pipeline\n",
    "- `num_parallel_calls` : to process each CSV file in parallel using map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9705606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_from_folder(folder_path, columns_to_use, batch_size):\n",
    "    # Create a dataset of file paths in the folder\n",
    "    file_paths_dataset = tf.data.Dataset.list_files(folder_path + '/*.csv')\n",
    "    \n",
    "    # Function to process a single CSV file\n",
    "    def process_file(file_path):\n",
    "        # Read CSV file\n",
    "        records = tf.data.experimental.CsvDataset(file_path, record_defaults=[tf.string]*len(columns_to_use), header=True)\n",
    "        \n",
    "        # Parse CSV records\n",
    "        parsed_records = records.map(lambda *x: dict(zip(columns_to_use, x)))\n",
    "        print(parsed_records)\n",
    "        \n",
    "        # Convert data types\n",
    "        for col in parsed_records:\n",
    "            if col in ['int64', 'int32']:\n",
    "                parsed_records[col] = tf.cast(parsed_records[col], tf.int32)\n",
    "            elif col == 'object':\n",
    "                parsed_records[col] = tf.cast(parsed_records[col], tf.string)\n",
    "            else:\n",
    "                parsed_records[col] = tf.cast(parsed_records[col], tf.float32)\n",
    "        \n",
    "        return parsed_records\n",
    "    \n",
    "    # Process each CSV file in parallel\n",
    "    datasets = file_paths_dataset.map(process_file, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Concatenate datasets from all CSV files into one dataset\n",
    "    combined_dataset = tf.data.experimental.sample_from_datasets(datasets)\n",
    "    \n",
    "    # Shuffle and batch the combined dataset\n",
    "    combined_dataset = combined_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "    \n",
    "    # Prefetch for performance optimization\n",
    "    combined_dataset = combined_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e0892ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: {key_id: (), drawing: (), word: ()}, types: {key_id: tf.string, drawing: tf.string, word: tf.string}>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_152/1188280286.py:21 process_file  *\n        parsed_records[col] = tf.cast(parsed_records[col], tf.float32)\n\n    TypeError: 'MapDataset' object is not subscriptable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_152/86664877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Preprocess data from folder and create TensorFlow dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Iterate over the dataset (for demonstration purposes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_152/1188280286.py\u001b[0m in \u001b[0;36mpreprocess_data_from_folder\u001b[0;34m(folder_path, columns_to_use, batch_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Process each CSV file in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_paths_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Concatenate datasets from all CSV files into one dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1861\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m       return ParallelMapDataset(\n\u001b[0m\u001b[1;32m   1864\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   5018\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5019\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5020\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   5021\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5022\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4218\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4219\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3148\u001b[0m          \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m     \"\"\"\n\u001b[0;32m-> 3150\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   3151\u001b[0m         *args, **kwargs)\n\u001b[1;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4193\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   4194\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4195\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4196\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4123\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4124\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4125\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4126\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4127\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /tmp/ipykernel_152/1188280286.py:21 process_file  *\n        parsed_records[col] = tf.cast(parsed_records[col], tf.float32)\n\n    TypeError: 'MapDataset' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder path, columns to use, and batch size\n",
    "columns_to_use = ['key_id', 'drawing', 'word']\n",
    "batch_size = 32\n",
    "\n",
    "# Preprocess data from folder and create TensorFlow dataset\n",
    "train_dataset = preprocess_data_from_folder(TRAIN_PATH, columns_to_use, batch_size)\n",
    "\n",
    "# Iterate over the dataset (for demonstration purposes)\n",
    "for batch in train_dataset.take(1):\n",
    "    # Accessing a batch of data\n",
    "    print(\"Batch:\", batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145cad4",
   "metadata": {},
   "source": [
    "- process_file 함수 cast 방식변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7986d81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_from_folder(folder_path, columns_to_use, batch_size):\n",
    "    # Create a dataset of file paths in the folder\n",
    "    file_paths_dataset = tf.data.Dataset.list_files(folder_path + '/*.csv')\n",
    "    \n",
    "    # Check if any files matched the pattern\n",
    "    if file_paths_dataset.cardinality().numpy() == 0:\n",
    "        raise ValueError(f\"No CSV files found in the folder: {folder_path}\")\n",
    "    \n",
    "    # Function to process a single CSV file\n",
    "    def process_file(file_path):\n",
    "        # Read CSV file\n",
    "        records = tf.data.experimental.CsvDataset(file_path, record_defaults=[tf.string]*len(columns_to_use), header=True)\n",
    "        \n",
    "        # Parse CSV records\n",
    "        parsed_records = records.map(lambda *x: dict(zip(columns_to_use, x)))\n",
    "        \n",
    "        # Convert data types\n",
    "        parsed_records = {col: tf.cast(parsed_records[col], tf.float32) if col != 'word' else parsed_records[col] for col in columns_to_use}\n",
    "        \n",
    "        return parsed_records\n",
    "    \n",
    "    # Process each CSV file in parallel\n",
    "    datasets = file_paths_dataset.map(process_file, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Concatenate datasets from all CSV files into one dataset\n",
    "    combined_dataset = tf.data.experimental.sample_from_datasets(datasets)\n",
    "    \n",
    "    # Shuffle and batch the combined dataset\n",
    "    combined_dataset = combined_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "    \n",
    "    # Prefetch for performance optimization\n",
    "    combined_dataset = combined_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8009e5b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_152/2068426100.py:18 process_file  *\n        parsed_records = {col: tf.cast(parsed_records[col], tf.float32) if col != 'word' else parsed_records[col] for col in columns_to_use}\n\n    TypeError: 'MapDataset' object is not subscriptable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_152/86664877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Preprocess data from folder and create TensorFlow dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data_from_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns_to_use\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Iterate over the dataset (for demonstration purposes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_152/1452639389.py\u001b[0m in \u001b[0;36mpreprocess_data_from_folder\u001b[0;34m(folder_path, columns_to_use, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Process each CSV file in parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_paths_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Concatenate datasets from all CSV files into one dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[1;32m   1861\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m       return ParallelMapDataset(\n\u001b[0m\u001b[1;32m   1864\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m           \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   5018\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5019\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5020\u001b[0;31m     self._map_func = StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m   5021\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5022\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   4216\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4218\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4219\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4220\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3148\u001b[0m          \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m     \"\"\"\n\u001b[0;32m-> 3150\u001b[0;31m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[0m\u001b[1;32m   3151\u001b[0m         *args, **kwargs)\n\u001b[1;32m   3152\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3114\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4193\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   4194\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4195\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4196\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4197\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   4123\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4124\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4125\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4126\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4127\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /tmp/ipykernel_152/2068426100.py:18 process_file  *\n        parsed_records = {col: tf.cast(parsed_records[col], tf.float32) if col != 'word' else parsed_records[col] for col in columns_to_use}\n\n    TypeError: 'MapDataset' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "# Specify the folder path, columns to use, and batch size\n",
    "columns_to_use = ['key_id', 'drawing', 'word']\n",
    "batch_size = 32\n",
    "\n",
    "# Preprocess data from folder and create TensorFlow dataset\n",
    "train_dataset = preprocess_data_from_folder(TRAIN_PATH, columns_to_use, batch_size)\n",
    "\n",
    "# Iterate over the dataset (for demonstration purposes)\n",
    "for batch in train_dataset.take(1):\n",
    "    # Accessing a batch of data\n",
    "    print(\"Batch:\", batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c763b1f",
   "metadata": {},
   "source": [
    "- GPT가 만든 코드를 그대로 사용하니 어느 부분에서 문제가 생겼는지 이해하기가 어려움\n",
    "- tf.data docs를 참고해서 이해가 되는 내용 기반으로 코드르 수정해봄\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab833de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "823px",
    "left": "236px",
    "top": "148px",
    "width": "214.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
